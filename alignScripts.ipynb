{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version - 0.1.9.8 (240304)\n",
    "Reordered the takesByLine dictionary for ease of use in jsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table.dataframe th {text-align: center;}\n",
       "table.dataframe td {text-align: left;}\n",
       "</style>\n",
       "from IPython.core.display import HTML\n",
       "\n",
       "\n",
       "HTML(\"\"\"\n",
       "<style>\n",
       ".dataframe td, .dataframe th {\n",
       "    max-width: None;\n",
       "    overflow: hidden;\n",
       "    text-overflow: ellipsis;\n",
       "    white-space: normal;\n",
       "    table.dataframe th {\n",
       "        text-align: center;\n",
       "    }\n",
       "    table.dataframe td {\n",
       "        text-align: left;\n",
       "    }\n",
       "}\n",
       "</style>\n",
       "\"\"\")\n",
       "\n",
       "import pandas as pd\n",
       "pd.set_option('display.max_rows', None)\n",
       "pd.set_option('display.max_columns', None)\n",
       "pd.set_option('display.width', None)\n",
       "pd.set_option('display.max_colwidth', None)\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table.dataframe th {text-align: center;}\n",
    "table.dataframe td {text-align: left;}\n",
    "</style>\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".dataframe td, .dataframe th {\n",
    "    max-width: None;\n",
    "    overflow: hidden;\n",
    "    text-overflow: ellipsis;\n",
    "    white-space: normal;\n",
    "    table.dataframe th {\n",
    "        text-align: center;\n",
    "    }\n",
    "    table.dataframe td {\n",
    "        text-align: left;\n",
    "    }\n",
    "}\n",
    "</style>\n",
    "\"\"\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/whisper/timing.py:57: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @numba.jit\n"
     ]
    }
   ],
   "source": [
    "\"\"\" ## Imports \"\"\"\n",
    "import nltk\n",
    "import pprint as pp\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Parsing Logic\n",
    "from Levenshtein import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "\n",
    "# Skipping In a For Loop\n",
    "from itertools import islice\n",
    "import collections\n",
    "\n",
    "# Whisper Transcription\n",
    "import whisper_timestamped as whisper\n",
    "import datetime\n",
    "\n",
    "def display_df(df):\n",
    "    display(df.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector=\"th\", props=[('text-align', 'center')])]))\n",
    "\n",
    "def print_bold(text):\n",
    "    display(Markdown('**' + text + '**'))\n",
    "\n",
    "def pprint(obj, width=100):\n",
    "    return pp.pprint(obj, width=width, compact=True, sort_dicts=False, underscore_numbers=True)\n",
    "\n",
    "def printright(string, value, padding=30):\n",
    "    x = string.rjust(padding, ' ')\n",
    "    print(f'{x} {value}')\n",
    "    \n",
    "def spacer(i):\n",
    "    hype = '-' * (i + 1)\n",
    "    printright(hype, value='', padding=30)\n",
    "    \n",
    "def dateTime(file_name):\n",
    "    now = datetime.datetime.now()\n",
    "    date_time_str = now.strftime(\"%y%m%d-%H%M\")    \n",
    "    return f\"{file_name}_{date_time_str}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Mod 5 Draft 2.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5222/42681 [00:55<06:45, 92.47frames/s] "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\" ## Word-Level Transcription \"\"\"\n",
    "def wlTranscribe (file_path):\n",
    "    audio = whisper.load_audio(file_path)\n",
    "    model = whisper.load_model(\"medium\", device=\"cpu\")\n",
    "    trans_base_dict = whisper.transcribe(model, audio, language=\"en\", beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), vad=\"auditok\",verbose=True)\n",
    "    \n",
    "    return trans_base_dict\n",
    "\n",
    "from deepmultilingualpunctuation import PunctuationModel\n",
    "def addPunct(text):\n",
    "    model = PunctuationModel()\n",
    "    result = model.restore_punctuation(text)\n",
    "    return result\n",
    "\n",
    "def toJson(dictionary, file_name, prefix=\"\"):\n",
    "    if prefix != \"\":\n",
    "        file_name_with_prefix = f\"{prefix}_{file_name}\"\n",
    "        file_name = file_name_with_prefix\n",
    "    file_name_with_date = dateTime(file_name)\n",
    "    file_name_with_date += \".json\"\n",
    "\n",
    "    with open(file_name_with_date, \"w\") as file:\n",
    "        json.dump(dictionary, file, indent=4)\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".m4a\"):  # Fix: Pass file types as a tuple\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            file_name = os.path.splitext(file)[0]\n",
    "\n",
    "            print(f\"Processing file: {file}\")\n",
    "            result = wlTranscribe(file_path)\n",
    "            toJson(result, file_name, prefix='ASR')\n",
    "\n",
    "\n",
    "# Specify the folder containing the audio files\n",
    "audio_folder = \"/Users/tristangardner/Documents/Suora/Suora Course/Mod 5 Dictation\"\n",
    "transcribe = True\n",
    "if transcribe == True: \n",
    "    process_folder(\"/Users/tristangardner/Documents/Suora/Suora Course/Mod 5 Dictation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ## Pre-Proccess Scripts \"\"\"\n",
    "def preprocess_script(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    text_start = text.find('\"')\n",
    "    text_end = text.rfind('\"')    \n",
    "    text = text[text_start+2:text_end-1]\n",
    "    \n",
    "    # Text to process\n",
    "    # Mapping of numbers to words\n",
    "    num_to_word = {'1': 'One', '2': 'Two', '3': 'Three', '4': 'Four', '5': 'Five', '6': 'Six', '7': 'Seven', '8': 'Eight', '9': 'Nine'}\n",
    "\n",
    "    # Replace single-digit numbers followed by a period with words\n",
    "    text = re.sub(r'\\b([1-9])\\.', lambda x: num_to_word[x.group(1)] + ',', text)\n",
    "    text = re.sub(r'[;:]', '.', text) # Replace semi-colons and colons with periods. \n",
    "\n",
    "            \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    #preprocessed_sentences = [re.sub(r'[^-\\w \\']', '', sentence).lower() for sentence in sentences]\n",
    "    #preprocessed_sentences = [re.sub(r'[\\n\\']', '', sentence) for sentence in sentences]\n",
    "    preprocessed_sentences = [re.sub(r'[\\u2022\\t]', ' ', sentence) for sentence in sentences] # Remove bullet points\n",
    "    preprocessed_sentences = [re.sub(r\"\\[.*?\\]\", '', sentence) for sentence in preprocessed_sentences] # Remove html tags\n",
    "\n",
    "    preprocessed_sentences = [re.sub(r'[\\n]', '', sentence).strip() for sentence in preprocessed_sentences] # Remove tabs\n",
    "\n",
    "    \n",
    "    for index, sentence in enumerate(preprocessed_sentences):\n",
    "        if index > 0:\n",
    "            if sentence[-1:] in [\"'\",'\"']:\n",
    "                preprocessed_sentences[index-1] = preprocessed_sentences[index-1] + ' ' + sentence\n",
    "                preprocessed_sentences[index] = None\n",
    "                \n",
    "    preprocessed_sentences = [sentence for sentence in preprocessed_sentences if sentence is not None]    \n",
    "    return preprocessed_sentences\n",
    "\n",
    "def preProcessScriptFolder(script_folder):\n",
    "    for file in os.listdir(scripts_folder):\n",
    "        base_path = scripts_folder + '/'\n",
    "        filepath = base_path + file\n",
    "        processed_script = preprocess_script(filepath)\n",
    "        \n",
    "        script_title = os.path.splitext(file)[0]\n",
    "        with open(filepath, 'w') as file:\n",
    "            file.write(script_title +'\\n\"\\n')\n",
    "            for sentence in processed_script:\n",
    "                file.write(sentence + '\\n')\n",
    "            file.write('\"')\n",
    "            \n",
    "# scripts_folder = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Scripts'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\ndef AAAAgetCutsByGroupAndSentence(input_dictionary):\\n    # df = pd.DataFrame(input_dictionary)\\n\\n    # output_dictionary = {\\n    #     'cut id': [],\\n    #     'start/end': [],\\n    #     'sentence indices': [],\\n    #     'sentences': [],\\n    #     'group ids': [],\\n    #     'similarity': [],\\n    #     'take indices': [],\\n    #     'asr matches': []\\n    # }\\n\\n    # cut_counter = 0\\n    # # Group by both 'Script Sentence Index' and 'Group ID'\\n    # grouped_df = df.groupby(['Script Sentence Index', 'Group ID'])\\n\\n    # for (script_sentence_index, group_id), group in grouped_df:\\n    #     # Get the start and end time for the group\\n    #     group_start = group['Start Time'].min()\\n    #     group_end = group['End Time'].max()\\n    #     similarity = group['Similarity Score'].mean()\\n\\n    #     # Collect all unique script sentences in the group\\n    #     unique_script_sentences = group['Script Sentence'].unique()\\n    #     combined_sentences = ' '.join(unique_script_sentences)\\n\\n    #     # Collect the take indices and their corresponding ASR matches\\n    #     take_indices = group['Take Index'].tolist()\\n    #     asr_matches = group['ASR Matches'].tolist()\\n\\n    #     # Increment the counter and append the values to the dictionary\\n    #     cut_counter += 1\\n    #     output_dictionary['cut id'].append(cut_counter)\\n    #     output_dictionary['start/end'].append((group_start, group_end))\\n    #     output_dictionary['sentence indices'].append(script_sentence_index)\\n    #     output_dictionary['sentences'].append(combined_sentences)\\n    #     output_dictionary['group ids'].append(group_id)\\n    #     output_dictionary['similarity'].append(similarity)\\n    #     output_dictionary['take indices'].append(take_indices)\\n    #     output_dictionary['asr matches'].append(asr_matches)\\n\\n    return output_dictionary\\n \""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ## Main Emeddings & Cutting Functions \"\"\"\n",
    "def get_bert_embeddings(string_sequence):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-cased'           #'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize and encode the sequence\n",
    "    tokens = tokenizer(string_sequence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Get the BERT embeddings for the sequence\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def getCuts(input_dictionary):\n",
    "\n",
    "    df = pd.DataFrame(input_dictionary)\n",
    "\n",
    "    output_dictionary = {\n",
    "        'cut id': [],\n",
    "        'start/end': [],\n",
    "        'sentence indices': [],\n",
    "        'sentences': [],\n",
    "        'group ids': [],\n",
    "        'similarity': [],\n",
    "        'take indices': [],\n",
    "        'asr matches': []\n",
    "    }\n",
    "\n",
    "    new_row = 0\n",
    "    cut_counter = 0\n",
    "    for i in range(len(df)):\n",
    "        if i == 0 or df.loc[i, 'Group ID'] != df.loc[i - 1, 'Group ID']:\n",
    "            new_row = i  # Marks the start of a new group\n",
    "\n",
    "        # When we get to the last row in a group or the end of the dataframe\n",
    "        if i == len(df) - 1 or df.loc[i, 'Group ID'] != df.loc[i + 1, 'Group ID']:\n",
    "            # Define the end of the group\n",
    "            group_end = i\n",
    "\n",
    "            # When at the end of a group, assign start and end times as a tuple\n",
    "            group_start_time = df.at[new_row, 'Start Time']\n",
    "            group_end_time = df.at[group_end, 'End Time']\n",
    "\n",
    "            # Collect all unique script sentences, take indices, and ASR matches in the group\n",
    "            unique_script_sentences = df.loc[new_row:group_end, 'Script Sentence'].unique()\n",
    "            take_indices = df.loc[new_row:group_end, 'Take Index'].tolist()\n",
    "            asr_matches = df.loc[new_row:group_end, 'ASR Matches'].tolist()\n",
    "            group_similarity = df.loc[new_row:group_end, 'Similarity Score'].mean()\n",
    "            group_id = df.at[new_row, 'Group ID']\n",
    "\n",
    "            # Increment the counter and append the values to the dictionary\n",
    "            cut_counter += 1\n",
    "            output_dictionary['cut id'].append(cut_counter)\n",
    "            output_dictionary['start/end'].append((group_start_time, group_end_time))\n",
    "            output_dictionary['sentence indices'].append(df.at[new_row, 'Script Sentence Index'])\n",
    "            output_dictionary['sentences'].append(' '.join(unique_script_sentences))\n",
    "            output_dictionary['group ids'].append(group_id)\n",
    "            output_dictionary['similarity'].append(group_similarity)\n",
    "            output_dictionary['take indices'].append(take_indices)\n",
    "            output_dictionary['asr matches'].append(asr_matches)\n",
    "\n",
    "    return output_dictionary\n",
    "\n",
    "\n",
    "#! testing reorder 240304\n",
    "def getCutsByGroupAndSentence(input_dictionary):\n",
    "    df = pd.DataFrame(input_dictionary)\n",
    "\n",
    "    output_dictionary = {}\n",
    "\n",
    "    cut_counter = 0\n",
    "    # Group by both 'Script Sentence Index' and 'Group ID'\n",
    "    grouped_df = df.groupby([\"Script Sentence Index\", \"Group ID\"])\n",
    "\n",
    "    for (script_sentence_index, group_id), group in grouped_df:\n",
    "        # Get the start and end time for the group\n",
    "        group_start = group[\"Start Time\"].min()\n",
    "        group_end = group[\"End Time\"].max()\n",
    "        similarity = group[\"Similarity Score\"].mean()\n",
    "\n",
    "        # Collect all unique script sentences in the group\n",
    "        unique_script_sentences = group[\"Script Sentence\"].unique()\n",
    "        combined_sentences = \" \".join(unique_script_sentences)\n",
    "\n",
    "        # Collect the take indices and their corresponding ASR matches\n",
    "        take_indices = group[\"Take Index\"].tolist()\n",
    "        asr_matches = group[\"ASR Matches\"].tolist()\n",
    "\n",
    "        # Increment the counter and append the values to the dictionary\n",
    "        cut_counter += 1\n",
    "        cut_info = {\n",
    "            \"cut id\": cut_counter,\n",
    "            \"start/end\": (group_start, group_end),\n",
    "            \"sentences\": combined_sentences,\n",
    "            \"group ids\": group_id,\n",
    "            \"similarity\": similarity,\n",
    "            \"take indices\": take_indices,\n",
    "            \"asr matches\": asr_matches,\n",
    "        }\n",
    "\n",
    "        # Convert script_sentence_index to string and add the cut info to the output dictionary, nested under the sentence index\n",
    "        script_sentence_index = str(script_sentence_index)\n",
    "        if script_sentence_index not in output_dictionary:\n",
    "            output_dictionary[script_sentence_index] = []\n",
    "        output_dictionary[script_sentence_index].append(cut_info)\n",
    "\n",
    "    # Sort the cuts within each sentence index group by similarity, from largest to smallest\n",
    "    for sentence_index, cuts in output_dictionary.items():\n",
    "        output_dictionary[sentence_index] = sorted(\n",
    "            cuts, key=lambda x: x[\"similarity\"], reverse=True\n",
    "        )\n",
    "\n",
    "    return output_dictionary\n",
    "\n",
    "\n",
    "#! save 240204\n",
    "\"\"\" \n",
    "def AAAAgetCutsByGroupAndSentence(input_dictionary):\n",
    "    # df = pd.DataFrame(input_dictionary)\n",
    "\n",
    "    # output_dictionary = {\n",
    "    #     'cut id': [],\n",
    "    #     'start/end': [],\n",
    "    #     'sentence indices': [],\n",
    "    #     'sentences': [],\n",
    "    #     'group ids': [],\n",
    "    #     'similarity': [],\n",
    "    #     'take indices': [],\n",
    "    #     'asr matches': []\n",
    "    # }\n",
    "\n",
    "    # cut_counter = 0\n",
    "    # # Group by both 'Script Sentence Index' and 'Group ID'\n",
    "    # grouped_df = df.groupby(['Script Sentence Index', 'Group ID'])\n",
    "\n",
    "    # for (script_sentence_index, group_id), group in grouped_df:\n",
    "    #     # Get the start and end time for the group\n",
    "    #     group_start = group['Start Time'].min()\n",
    "    #     group_end = group['End Time'].max()\n",
    "    #     similarity = group['Similarity Score'].mean()\n",
    "\n",
    "    #     # Collect all unique script sentences in the group\n",
    "    #     unique_script_sentences = group['Script Sentence'].unique()\n",
    "    #     combined_sentences = ' '.join(unique_script_sentences)\n",
    "\n",
    "    #     # Collect the take indices and their corresponding ASR matches\n",
    "    #     take_indices = group['Take Index'].tolist()\n",
    "    #     asr_matches = group['ASR Matches'].tolist()\n",
    "\n",
    "    #     # Increment the counter and append the values to the dictionary\n",
    "    #     cut_counter += 1\n",
    "    #     output_dictionary['cut id'].append(cut_counter)\n",
    "    #     output_dictionary['start/end'].append((group_start, group_end))\n",
    "    #     output_dictionary['sentence indices'].append(script_sentence_index)\n",
    "    #     output_dictionary['sentences'].append(combined_sentences)\n",
    "    #     output_dictionary['group ids'].append(group_id)\n",
    "    #     output_dictionary['similarity'].append(similarity)\n",
    "    #     output_dictionary['take indices'].append(take_indices)\n",
    "    #     output_dictionary['asr matches'].append(asr_matches)\n",
    "\n",
    "    return output_dictionary\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCutsByGroupAndSentence(input_dictionary):\n",
    "    df = pd.DataFrame(input_dictionary)\n",
    "\n",
    "    output_dictionary = {}\n",
    "\n",
    "    cut_counter = 0\n",
    "    # Group by both 'Script Sentence Index' and 'Group ID'\n",
    "    grouped_df = df.groupby([\"Script Sentence Index\", \"Group ID\"])\n",
    "\n",
    "    for (script_sentence_index, group_id), group in grouped_df:\n",
    "        # Get the start and end time for the group\n",
    "        group_start = group[\"Start Time\"].min()\n",
    "        group_end = group[\"End Time\"].max()\n",
    "        similarity = group[\"Similarity Score\"].mean()\n",
    "\n",
    "        # Collect all unique script sentences in the group\n",
    "        unique_script_sentences = group[\"Script Sentence\"].unique()\n",
    "        combined_sentences = \" \".join(unique_script_sentences)\n",
    "\n",
    "        # Collect the take indices and their corresponding ASR matches\n",
    "        take_indices = group[\"Take Index\"].tolist()\n",
    "        asr_matches = group[\"ASR Matches\"].tolist()\n",
    "\n",
    "        # Increment the counter and append the values to the dictionary\n",
    "        cut_counter += 1\n",
    "        cut_info = {\n",
    "            \"cut id\": cut_counter,\n",
    "            \"start/end\": (group_start, group_end),\n",
    "            \"sentences\": combined_sentences,\n",
    "            \"group ids\": group_id,\n",
    "            \"similarity\": similarity,\n",
    "            \"take indices\": take_indices,\n",
    "            \"asr matches\": asr_matches,\n",
    "        }\n",
    "\n",
    "        # Convert script_sentence_index to string and add the cut info to the output dictionary, nested under the sentence index\n",
    "        script_sentence_index = str(script_sentence_index)\n",
    "        if script_sentence_index not in output_dictionary:\n",
    "            output_dictionary[script_sentence_index] = []\n",
    "        output_dictionary[script_sentence_index].append(cut_info)\n",
    "\n",
    "    # Sort the cuts within each sentence index group by similarity, from largest to smallest\n",
    "    for sentence_index, cuts in output_dictionary.items():\n",
    "        output_dictionary[sentence_index] = sorted(\n",
    "            cuts, key=lambda x: x[\"similarity\"], reverse=True\n",
    "        )\n",
    "\n",
    "    return output_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renameFile(folder_path, old_string, new_string):\n",
    "    import os\n",
    "    for file in os.listdir(folder_path):\n",
    "        if old_string in file:\n",
    "            old_file = os.path.join(folder_path, file)\n",
    "            new_file = old_file.replace(old_string, new_string)\n",
    "            updated_file = os.path.join(folder_path, new_file)\n",
    "            os.rename(old_file, updated_file)\n",
    "\n",
    "folder_path = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/Bio.txt</th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1A.txt</th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1B.txt</th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1C.txt</th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1D.txt</th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1E.txt</th>\n",
       "      <th>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1F.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_Bio_1_proxyWT_240117-2300.json</td>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1A_1_proxyWT_240117-2303.json</td>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1B_1_proxyWT_240117-2325.json</td>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1C_1_proxyWT_240117-2344.json</td>\n",
       "      <td>None</td>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1E_1_proxyWT_240117-2358.json</td>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1F_1_proxyWT_240118-2353.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_Bio_2_proxyWT_240117-2259.json</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1C_2_proxyWT_240117-2342.json</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_Bio_3_proxyWT_240117-2300.json</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/Bio.txt  \\\n",
       "0  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_Bio_1_proxyWT_240117-2300.json   \n",
       "1  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_Bio_2_proxyWT_240117-2259.json   \n",
       "2  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_Bio_3_proxyWT_240117-2300.json   \n",
       "\n",
       "                       /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1A.txt  \\\n",
       "0  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1A_1_proxyWT_240117-2303.json   \n",
       "1                                                                                                                 None   \n",
       "2                                                                                                                 None   \n",
       "\n",
       "                       /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1B.txt  \\\n",
       "0  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1B_1_proxyWT_240117-2325.json   \n",
       "1                                                                                                                 None   \n",
       "2                                                                                                                 None   \n",
       "\n",
       "                       /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1C.txt  \\\n",
       "0  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1C_1_proxyWT_240117-2344.json   \n",
       "1  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1C_2_proxyWT_240117-2342.json   \n",
       "2                                                                                                                 None   \n",
       "\n",
       "  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1D.txt  \\\n",
       "0                                                                                            None   \n",
       "1                                                                                            None   \n",
       "2                                                                                            None   \n",
       "\n",
       "                       /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1E.txt  \\\n",
       "0  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1E_1_proxyWT_240117-2358.json   \n",
       "1                                                                                                                 None   \n",
       "2                                                                                                                 None   \n",
       "\n",
       "                       /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1F.txt  \n",
       "0  /Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1F_1_proxyWT_240118-2353.json  \n",
       "1                                                                                                                 None  \n",
       "2                                                                                                                 None  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ## (1) Input Files, sort and group ASR files by script \"\"\"\n",
    "##! asr files and script files must include the same string in their names to be matched\n",
    "##! multiple asr files for one script should have a counter\n",
    "\n",
    "##* these paths will be taken from the html panel browse dialog and fed to this python script\n",
    "asr_folder = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR'\n",
    "scripts_folder = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts'\n",
    "\n",
    "file_dict = {}\n",
    "\n",
    "for script_file in os.listdir(scripts_folder):\n",
    "    script_file_name = os.path.splitext(script_file)[0]\n",
    "    script_file_path = os.path.join(scripts_folder, script_file)\n",
    "    file_dict[script_file_path] = []\n",
    "    # print(f'ASR files for script: {script_file_name}')\n",
    "    \n",
    "    for asr_file in os.listdir(asr_folder):\n",
    "        asr_file_name = os.path.splitext(asr_file)[0]\n",
    "        asr_file_path = os.path.join(asr_folder, asr_file)\n",
    "        if script_file_name in asr_file_name:\n",
    "            file_dict[script_file_path].append(asr_file_path)\n",
    "            # print(f'\\tASR File Path: {asr_file_path}')            \n",
    "    # print('\\n')\n",
    "    \n",
    "file_dict = dict(sorted(file_dict.items()))\n",
    "file_dict_df = pd.DataFrame.from_dict(file_dict, orient='index').transpose()\n",
    "file_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTakes(script_sentences, asr_file_path, debug=False):\n",
    "    ##* returns a dictionary of cuts, each cell has objects converted to strings for JavaScript ingest\n",
    "\n",
    "    print_and_display = debug\n",
    "\n",
    "    ## CORPUS CREATION\n",
    "    #! each script must begin with the title of the script in one line, and a \" in the next line - the script should start on the next line - it should have another \" after the last script line\n",
    "\n",
    "    with open (asr_file_path, 'r') as file:\n",
    "        asr_dict = json.load(file)\n",
    "\n",
    "    # Initialize variables for sentence assembly\n",
    "    asr_sentences           = []\n",
    "    sentence_start_times    = []\n",
    "    sentence_end_times      = []\n",
    "    current_sentence        = ''\n",
    "    current_start_time      = None\n",
    "\n",
    "    # Iterate through segments\n",
    "    for segment in asr_dict['segments']:\n",
    "        words = segment['words']\n",
    "\n",
    "        for word in words:\n",
    "            word_text = word['text']\n",
    "            word_start_time = word['start']\n",
    "            word_end_time = word['end']\n",
    "\n",
    "            if not current_sentence:\n",
    "                # Start a new sentence\n",
    "                current_sentence = word_text\n",
    "                current_start_time = word_start_time\n",
    "            else:\n",
    "                # Append word to current sentence\n",
    "                current_sentence += ' ' + word_text\n",
    "\n",
    "            # Check if the word ends with punctuation\n",
    "            if any(punct in word_text for punct in ['.', '?', '!']):\n",
    "                # End of the current sentence\n",
    "                asr_sentences.append(current_sentence)\n",
    "                sentence_start_times.append(current_start_time)\n",
    "                sentence_end_times.append(word_end_time)\n",
    "\n",
    "                # Reset for the next sentence\n",
    "                current_sentence = ''\n",
    "                current_start_time = None\n",
    "\n",
    "    # Handle case where the last word does not end with punctuation\n",
    "    if current_sentence:\n",
    "        asr_sentences.append(current_sentence)\n",
    "        sentence_end_times.append(segment['end'])\n",
    "\n",
    "    # Combine sentences with their start and end times\n",
    "    assembled_sentences     = [{\"Sentence\": sentence, \"Start Time\": start, \"End Time\": end}\n",
    "                        for sentence, start, end in zip(asr_sentences, sentence_start_times, sentence_end_times)]\n",
    "\n",
    "    df_assembled_sentences  = pd.DataFrame(assembled_sentences)\n",
    "    # df_assembled_sentences\n",
    "\n",
    "    # Find the maximum length among the lists\n",
    "    max_length              = max(len(asr_sentences), len(sentence_start_times), len(sentence_end_times), len(script_sentences)) ##! fix script sentences first to fill itself from script file \n",
    "    script_sentences.extend([None] * (max_length - len(script_sentences)))\n",
    "    asr_sentences.extend([None] * (max_length - len(asr_sentences)))\n",
    "    sentence_start_times.extend([None] * (max_length - len(sentence_start_times)))\n",
    "    sentence_end_times.extend([None] * (max_length - len(sentence_end_times)))\n",
    "\n",
    "    \"\"\" \n",
    "    asr_segments.extend([None] * (max_length - len(asr_segments)))\n",
    "    segment_start_times.extend([None] * (max_length - len(segment_start_times)))\n",
    "    segment_end_times.extend([None] * (max_length - len(segment_end_times))) \"\"\"\n",
    "\n",
    "    dict_corpus = {\n",
    "        'Script Sentences': script_sentences,\n",
    "        'Sentence': asr_sentences, \n",
    "        'Sentence Start': sentence_start_times, \n",
    "        'Sentence End': sentence_end_times,\n",
    "    }\n",
    "\n",
    "    df_corpus = pd.DataFrame(dict_corpus, columns=['Script Sentences', 'Sentence', 'Sentence Start', 'Sentence End'])\n",
    "    if print_and_display == True:\n",
    "        print('\\n********** Corpus DF **********\\n')\n",
    "        display(df_corpus)\n",
    "\n",
    "    \"\"\" ## SENTENCE SIMILARITY UPSTREAM \"\"\"\n",
    "    results = {\n",
    "        'Matches': {\n",
    "        'Script Sentence Index': [],\n",
    "        'Script Sentence': [],\n",
    "        'Take Index': [], # ASR Segment Index\n",
    "        'ASR Matches': [],\n",
    "        'Similarity Score': [],\n",
    "        'Start Time': [],\n",
    "        'End Time': []\n",
    "        },\n",
    "        'Unmatched': {}\n",
    "    }\n",
    "\n",
    "    # region: Generate Embeddings\n",
    "    transformer = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n",
    "    model = SentenceTransformer(transformer)\n",
    "\n",
    "    sentences1 = [sentence for sentence in script_sentences if sentence is not None]\n",
    "    sentences2 = [sentence for sentence in asr_sentences if sentence is not None]\n",
    "\n",
    "    embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "    # endregion: Generate Embeddings\n",
    "\n",
    "    for i, embedding1 in enumerate(embeddings1):\n",
    "        # Compute cosine similarities\n",
    "        cosine_scores = util.cos_sim(embedding1, embeddings2)\n",
    "        cosine_scores = cosine_scores.flatten()\n",
    "\n",
    "        # Clamp the cosine scores above threshold\n",
    "        valid_indices = np.where(cosine_scores > 0.65)[0]\n",
    "        valid_scores = cosine_scores[valid_indices]\n",
    "\n",
    "        if len(valid_scores) > 0:\n",
    "            percentile = np.percentile(valid_scores, 90) # this doesn't do much if multiple takes have the same exact words --> group window weight is needed\n",
    "\n",
    "            # Find indices where the cosine score is equal to or higher than the 70th percentile\n",
    "            top_indices = valid_indices[np.where(valid_scores >= percentile)[0]]\n",
    "            top_scores = cosine_scores[top_indices]\n",
    "\n",
    "            # Append the results to your dictionary\n",
    "            for index, score in zip(top_indices, top_scores):\n",
    "                match_data = (i, sentences1[i], index, sentences2[index], score.item(), sentence_start_times[index], sentence_end_times[index])\n",
    "                for key, value in zip(results['Matches'].keys(), match_data):\n",
    "                    results['Matches'][key].append(value)\n",
    "        else:\n",
    "            # Handle case with no valid matches\n",
    "            results['Matches']['Script Sentence Index'].append(i)\n",
    "            results['Matches']['Script Sentence'].append(sentences1[i])\n",
    "            results['Matches']['Take Index'].append(None)\n",
    "            results['Matches']['ASR Matches'].append(\"No Matches Found\")\n",
    "            results['Matches']['Similarity Score'].append(None)\n",
    "            results['Matches']['Start Time'].append(None)\n",
    "            results['Matches']['End Time'].append(None)\n",
    "\n",
    "    df_results = pd.DataFrame(results['Matches'])\n",
    "    if print_and_display == True:\n",
    "        print('\\n********** Results DF **********\\n')\n",
    "        display(df_results)\n",
    "\n",
    "    \"\"\" ## Sort by Start Time \"\"\"\n",
    "    df_results_1_sortStart = df_results.copy()\n",
    "    df_results_1_sortStart = df_results_1_sortStart.sort_values(by=['Start Time'])\n",
    "    df_results_1_sortStart.reset_index(drop=True)\n",
    "    \"\"\"## Grouping Sentences into Time Windows  \"\"\"\n",
    "    df_results_2_grouped = df_results_1_sortStart.copy()\n",
    "\n",
    "    # Initialize a list to keep track of each group's time window\n",
    "    group_windows = []\n",
    "\n",
    "    for i in range(len(df_results_2_grouped)):\n",
    "        start_time = df_results_2_grouped.at[i, 'Start Time']\n",
    "        if start_time == None:\n",
    "            continue\n",
    "        end_time = df_results_2_grouped.at[i, 'End Time']\n",
    "        assigned = False\n",
    "\n",
    "        # Check if the sentence fits into the time window of any existing group\n",
    "        for group_id, (group_start, group_end) in enumerate(group_windows):\n",
    "            if max(group_start, start_time) - min(group_end, end_time) <= 3.5: #! @Param: Time Window\n",
    "                # Update the group's time window\n",
    "                group_windows[group_id] = (min(group_start, start_time), max(group_end, end_time))\n",
    "                df_results_2_grouped.at[i, 'Group ID'] = chr(65 + group_id)  # Convert group_id to a letter\n",
    "                assigned = True\n",
    "                break\n",
    "\n",
    "        if not assigned:\n",
    "            # Start a new group and assign a letter as ID\n",
    "            new_group_id = len(group_windows)\n",
    "            df_results_2_grouped.at[i, 'Group ID'] = chr(65 + new_group_id)  # Convert new_group_id to a letter\n",
    "            group_windows.append((start_time, end_time))\n",
    "\n",
    "    df_results_2_grouped.reset_index(drop=True)\n",
    "    ## Group Sizesx\n",
    "    # Group by 'Group ID' and count the size of each group\n",
    "    group_sizes = df_results_2_grouped.groupby('Group ID').size()\n",
    "\n",
    "    # Map the group size back to the original DataFrame\n",
    "    df_results_2_grouped['Group Size'] = df_results_2_grouped['Group ID'].map(group_sizes)\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    df_results_2_grouped.sort_values(by=['Start Time'])\n",
    "    if print_and_display == True:\n",
    "        print('\\n********** Grouped DF **********\\n')\n",
    "        display(df_results_2_grouped)\n",
    "\n",
    "    ## Create Final dict\n",
    "\n",
    "    df_results_grouped_2_dropDuplicates = df_results_2_grouped.sort_values(by=['Start Time']).drop_duplicates(subset='Take Index')\n",
    "\n",
    "    results_dict = df_results_grouped_2_dropDuplicates.dropna().to_dict('records')\n",
    "\n",
    "    # df_results_grouped_D_dropDuplicates\n",
    "\n",
    "    matches = {\n",
    "        'All Takes': {}, ## DONE \n",
    "        'Longest Takes': {}, # Each asr match for every script sentence index that has the longest associated group size\n",
    "        'Highest Score Takes': {},\n",
    "        'Last Takes': {}, # the last-most take of each script sentence\n",
    "        'Script Sentence Takes': {} # all matches for each script sentence \n",
    "        #'Add Libs': {} # just the ones that are inside of a long take?\n",
    "    }\n",
    "\n",
    "    matches['All Takes'] = results_dict\n",
    "\n",
    "    ## Filling add Group Start and Group End to each dictionary in 'All Takes'\n",
    "    # Add 'Group Start' and 'Group End' to each dictionary in 'All Takes'\n",
    "    for take in matches['All Takes']:\n",
    "        # Find the group that this take belongs to\n",
    "        group = df_results_grouped_2_dropDuplicates[df_results_grouped_2_dropDuplicates['Group ID'] == take['Group ID']]\n",
    "        # Find the 'Start Time' of the take with the lowest start time in the group\n",
    "        group_start = group['Start Time'].min()\n",
    "        # Find the 'End Time' of the take with the highest end time in the group\n",
    "        group_end = group['End Time'].max()\n",
    "        # Add 'Group Start' and 'Group End' to the dictionary\n",
    "        take['Group Start'] = group_start\n",
    "        take['Group End'] = group_end\n",
    "\n",
    "    ## Filling in Longest Takes with each script sentence match that is associated with the biggest group of which that take is a part\n",
    "    # Convert 'All Takes' to a DataFrame\n",
    "    df_all_takes = pd.DataFrame(matches['All Takes'])\n",
    "    # Group by 'Script Sentence Index' and find the dictionary with the highest group size in each group\n",
    "    df_longest_takes = df_all_takes.loc[df_all_takes.groupby('Script Sentence Index')['Group Size'].idxmax()]\n",
    "    # Convert 'Longest Takes' back to a list of dictionaries\n",
    "    matches['Longest Takes'] = df_longest_takes.to_dict('records')\n",
    "\n",
    "    ## Filling highest score takes\n",
    "    df_highest_score_takes = df_all_takes.loc[df_all_takes.groupby('Script Sentence Index')['Similarity Score'].idxmax()]\n",
    "    matches['Highest Score Takes'] = df_highest_score_takes.to_dict('records')\n",
    "\n",
    "    ## Filling latest recorded takes\n",
    "    df_latest_recorded_takes = df_all_takes.loc[df_all_takes.groupby('Script Sentence Index')['End Time'].idxmax()]\n",
    "    matches['Last Takes'] = df_latest_recorded_takes.to_dict('records')\n",
    "\n",
    "    ## Filling script sentence matches\n",
    "    df_script_sentence_matches = df_all_takes.sort_values(by=['Script Sentence Index'])\n",
    "    matches['Script Sentence Takes'] = df_script_sentence_matches.to_dict('records')\n",
    "    if print_and_display == True:\n",
    "        print('\\n********** Script Sentence Matches DF **********\\n')\n",
    "        display(df_script_sentence_matches)\n",
    "\n",
    "    ## Create Final cuts dict & JSON\n",
    "    cuts = {}\n",
    "\n",
    "    cuts['Longest Takes'] = getCuts(matches['Longest Takes'])\n",
    "    cuts['Highest Score Takes'] = getCuts(matches['Highest Score Takes'])\n",
    "    cuts['Last Takes'] = getCuts(matches['Last Takes'])\n",
    "    cuts['Script Sentence Takes'] = getCutsByGroupAndSentence(matches['Script Sentence Takes'])\n",
    "\n",
    "    # cuts_df = pd.DataFrame(cuts, columns=['Longest Takes', 'Highest Score Takes', 'Last Takes', 'Script Sentence Takes'])\n",
    "    # if print_and_display == True:\n",
    "    #     print('\\n********** Cuts DF **********\\n')\n",
    "    #     display(cuts_df)\n",
    "\n",
    "    ## Convert Keys/Values in Cuts Dict so so JSON.dump ingests all correct object types\n",
    "    def convert_numpy_int_to_python_int(obj):\n",
    "        if isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_numpy_int_to_python_int(item) for item in obj]\n",
    "        elif isinstance(obj, tuple):\n",
    "            return tuple(convert_numpy_int_to_python_int(item) for item in obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: convert_numpy_int_to_python_int(value) for key, value in obj.items()}\n",
    "        return obj\n",
    "\n",
    "    # Apply this function to your dictionary\n",
    "    converted_cuts_dict = convert_numpy_int_to_python_int(cuts)\n",
    "\n",
    "    return converted_cuts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ## Main Folder Run \"\"\"\n",
    "\n",
    "cuts_export_folder = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Cuts'\n",
    "\n",
    "for column in file_dict_df:\n",
    "    script_file_path = column\n",
    "    with open(script_file_path, 'r') as file:\n",
    "        script_sentences = file.read().splitlines()\n",
    "        script_sentences = script_sentences[2:-1] ##* where script_sentences is finalized for use\n",
    "        \n",
    "    for index, value in file_dict_df[column].items():\n",
    "        if pd.notna(value):  # Check if the value is not NaN (None in your case)\n",
    "            asr_file_path = value\n",
    "            converted_cuts_dict = createTakes(script_sentences, asr_file_path)\n",
    "            cuts_file_script = os.path.splitext(os.path.basename(script_file_path))[0]\n",
    "            cuts_file_name = f'{cuts_file_script}_{index+1}'\n",
    "            toJson(converted_cuts_dict, cuts_file_name, prefix='cuts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ## Main Single File Run \"\"\"\n",
    "asr_E1D = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1D_1_proxyWT_240128-2114.json'\n",
    "asr_E1E = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/ASR/ASR_EXO_WM_E1E_1_proxyWT_240117-2358.json'\n",
    "\n",
    "asr_file_path = asr_E1E\n",
    "script_file_path = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Processed Scripts/E1E.txt'\n",
    "\n",
    "with open(script_file_path, 'r') as file:\n",
    "    script_sentences = file.read().splitlines()\n",
    "    script_sentences = script_sentences[2:-1] ##* where script_sentences is finalized for use\n",
    "\n",
    "converted_cuts_dict = createTakes(script_sentences, asr_file_path)\n",
    "cuts_file_script = os.path.splitext(os.path.basename(script_file_path))[0]\n",
    "cuts_file_name = cuts_file_script\n",
    "toJson(converted_cuts_dict, cuts_file_name, prefix='cuts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a cuts file to csv just for longest takes, which I don't really need to be doing\n",
    "from jsonToCSV import jsonToCSV\n",
    "\n",
    "file_path = '/Users/tristangardner/Documents/Programming/3. Test Media/Wayne Mayer/Cuts/cuts_E1D_1_240128-2143.json'\n",
    "key = 'Longest Takes'\n",
    "jsonToCSV(file_path, key, '_Longest Takes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
